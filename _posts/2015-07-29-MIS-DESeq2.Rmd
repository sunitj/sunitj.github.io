---
title: "MIS-DESeq2"
author: "Sunit Jain"
date: "January 6, 2015"
output:
  pdf_document:
    fig_crop: no
    toc: yes
    toc_depth: 3
  html_document:
    dev: svg
    toc: yes
    toc_depth: 3
categories: Differential Expression
---

\newpage

## Dependencies
If you're unsure that you have all the pacakges required to run this workflow. Open the `Rmd` file in your favorite text editor (I used [RStudio](http://www.rstudio.com)) and change the next line from `eval=FALSE` to `eval=TRUE`. Now, when you run this workflow, the dependencies should be installed first.

```{r install, eval=FALSE, results='hide',  echo=FALSE}
package = function(p) {
  if (!p %in% installed.packages()){ 
  		install.packages(p)
  }
#	require(p, character.only=TRUE)
}
package("gplots")
package("RColorBrewer")
package("PoiClaClu")
package("ggplot2")
package("knitr")
package("vioplot")
package("pheatmap")
package("ggdendro")
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
											echo=FALSE, warning=FALSE, message=FALSE)
```

```{r setup,  echo=FALSE}
## Setup
library("DESeq2")
workdir=getwd()
dataDir=paste(getwd(),"data", sep = "/")
gffFile=list.files(path=dataDir, pattern="*.gff", full.names=TRUE)
minRawCount=2 # at least 2 conditions should have at least this raw count to consider that gene
topN=500 # number of genes in rank abundance plote
```

## Generate a read count matrix using `htseq-count`
Sample command:

`htseq-count -f bam -r name -t CDS -o scaffold.htseq.sam -i ID -q scaffold_sortedByName.bam all_combined.gff`

This command was run for each sample individually.

### Merging duplicate genes
I performed a self blast and looked at results that had a percent identity greater than 98%, query coverage greater than 96% and a minimum alignment length of 500 bases. Once I had this subset, I screened out the hits to exons since we won't be considering them for this experiment anyway. I was left with the following two gene pairs:

* scaffold_344578__MIS_1109813.1  scaffold_133898__MIS_10093600.14
* scaffold_219988__MIS_10179608.12  scaffold_555373__MIS_1172265.1

that had high enough similarity based on the thresholds mentioned above that their count data needed to be merged. The perl script `mergeCounts.pl` was run on each htseq-count output individually in order to accomplish this. Here is a sample command used for one of the htseq-count outputs:

`perl mergeCounts.pl -l realDuplicateGenes.list -tsv Day_1.htseqCount.tsv -o Day_1.htseqCount.merged.tsv`

where, realDuplicateGenes.list contains the two gene pairs mentioned above.


```{r readTSV, }
### Import Count files
sampleFiles=list.files(path=dataDir,pattern="*.htseqCount.merged.tsv", full.names=TRUE)
```

```{r conditions, }
### Set Conditions
sampleCondition=c(rep("Day",3),rep("Night",3))
sampleName=c("Day_1","Day_2","Day_3", "Night_4", "Night_5", "Night_6")
sampleTable=data.frame(sampleName = sampleName, fileName=sampleFiles, condition=sampleCondition)
```

### Import Counts into DESeq2
Once we were satisfied with the genes and their counts. We imported the count data into DESeq2.
```{r deseq, cache=TRUE}
ddsHTSeq.all=DESeqDataSetFromHTSeqCount(sampleTable=sampleTable,design= ~ condition)
colData(ddsHTSeq.all)$condition<-factor(colData(ddsHTSeq.all)$condition, levels=c("Night","Day"))
```

## Reads per Sample
```{r readsPerSample}
colSums(counts(ddsHTSeq.all))
```

### Filtering the data
Get rid of genes which did not occur frequently enough. Here we say, lets get rid of genes with counts >=`r minRawCount` in at least 2 samples.
```{r dim}
keep=rowSums(counts(ddsHTSeq.all)>=minRawCount) >= 2
ddsHTSeq=ddsHTSeq.all[keep ,]
colSums(counts(ddsHTSeq))
```

### How many reads were removed when Min Raw Count = `r minRawCount`?
```{r difference}
colSums(counts(ddsHTSeq.all))-colSums(counts(ddsHTSeq))
```
This reduces the dataset from `r dim(counts(ddsHTSeq.all))[1]` tags to about `r dim(counts(ddsHTSeq))[1]`. For the filtered tags, there is very little power to detect differential expression, so little information is lost by filtering. 

\newpage

## Exploring the Dataset
### The `rlog` transformation
Many common statistical methods for exploratory analysis of multidimensional data, especially methods for clustering and ordination (e.g., principal-component analysis and the like), work best for (at least approximately) homoskedastic data; this means that the variance of an observed quantity (here, the expression strength of a gene) does not depend on the mean. In RNA-Seq data, however, variance grows with the mean. For example, if one performs PCA (principal components analysis) directly on a matrix of normalized read counts, the result typically depends only on the few most strongly expressed genes because they show the largest absolute differences between samples. A simple and often used strategy to avoid this is to take the logarithm of the normalized count values plus a small pseudocount; however, now the genes with low counts tend to dominate the results because, due to the strong Poisson noise inherent to small count values, they show the strongest relative differences between samples.

As a solution, DESeq2 offers the regularized-logarithm transformation, or rlog for short. For genes with high counts, the rlog transformation differs not much from an ordinary log2 transformation. For genes with lower counts, however, the values are shrunken towards the genes' averages across all samples. Using an empirical Bayesian prior on inter-sample differences in the form of a ridge penalty, this is done such that the rlog-transformed data are approximately homoskedastic.

**Note:** the rlog transformation is provided for applications other than differential testing. For differential testing we recommend the DESeq function applied to raw counts, as described later in this workflow, which also takes into account the dependence of the variance of counts on the mean value during the dispersion estimation step.

```{r rlog }
rld <- rlog(ddsHTSeq)
```

### Sample distances
A useful first step in an RNA-Seq analysis is often to assess overall similarity between samples: Which samples are similar to each other, which are different? Does this fit to the expectation from the experiment's design?
We use the R function `dist` to calculate the Euclidean distance between samples. To avoid that the distance measure is dominated by a few highly variable genes, and have a roughly equal contribution from all genes, we use it on the rlog-transformed data:
```{r dist}
sampleDists <- dist( t( assay(rld) ) )
sampleDists
```

We visualize the distances in a heatmap:

```{r distHeat, }
library("RColorBrewer")
library("pheatmap")
sampleDistMatrix <- as.matrix( sampleDists )
rownames(sampleDistMatrix) <- sampleTable$sampleName
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
hc <- hclust(sampleDists)
pheatmap(sampleDistMatrix,
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors)
```

\newpage

### Poisson Distance
Another option for calculating sample distances is to use the Poisson Distance, implemented in the CRAN package PoiClaClu. Similar to the transformations offered in DESeq2, this measure of dissimilarity also takes the variance structure of counts into consideration when calculating the distances between samples. The PoissonDistance function takes the original count matrix (not normalized) with samples as rows instead of columns, so we need to transpose the counts in dds.
```{r poissonDist}
library("PoiClaClu")
poisd <- PoissonDistance(t(counts(ddsHTSeq)))
samplePoisDistMatrix <- as.matrix( poisd$dd )
rownames(samplePoisDistMatrix) <-  sampleTable$sampleName
colors <- colorRampPalette( rev(brewer.pal(9, "Blues")) )(255)
hc <- hclust(poisd$dd)
pheatmap(samplePoisDistMatrix,
         clustering_distance_rows=poisd$dd,
         clustering_distance_cols=poisd$dd,
         col=colors)
```

\newpage

### PCA plot

Another way to visualize sample-to-sample distances is a principal-components analysis (PCA). In this ordination method, the data points (i.e., here, the samples) are projected onto the 2D plane such that they spread out in the two directions which explain most of the differences in the data. The x-axis is the direction (or principal component) which separates the data points the most. The amount of the total variance which is contained in the direction is printed in the axis label.
```{r pca, }
library("ggplot2")
data = plotPCA(rld, intgroup = "condition", returnData=TRUE)
percentVar = round(100 * attr(data, "percentVar"))

ggplot(data, aes(PC1, PC2)) +
	geom_point(aes(color=condition,shape=condition), size=5) +
	scale_color_brewer(palette = "Dark2") +
	xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
	geom_text(aes(label=name), hjust=0.5, vjust=-1.5) +
	theme_minimal()
```

```{r distDendro}
library(ggdendro)
hc=hclust(sampleDists)
ggdendrogram(hc,leaf_labels = T)
```

\newpage

### MDS plot

Another plot, very similar to the PCA plot, can be made using the multidimensional scaling (MDS) function in base R. This is useful when we don't have the original data, but only a matrix of distances. Here we have the MDS plot for the distances calculated from the rlog transformed counts:
```{r mds_dist, }
mds <- data.frame(cmdscale(sampleDistMatrix))
mds <- cbind(mds, colData(rld))
mds$name=sampleTable$sampleName
mds=data.frame(mds)

ggplot(mds, aes(X1, X2)) +
	geom_point(aes(color=condition,shape=condition), size=5) +
	scale_color_brewer(palette = "Dark2") +
	geom_text(aes(label=name), hjust=0.5, vjust=-1.5) +
	theme_minimal()
```

\newpage

And here from the PoissonDistance:

```{r mds_pd, }
mds <- data.frame(cmdscale(samplePoisDistMatrix))
mds <- cbind(mds, colData(ddsHTSeq))
mds$name=sampleTable$sampleName
mds=data.frame(mds)

ggplot(mds, aes(X1, X2)) +
	geom_point(aes(color=condition,shape=condition), size=5) +
	scale_color_brewer(palette = "Dark2") +
	geom_text(aes(label=name), hjust=0.5, vjust=-1.5) +
	theme_minimal()
```

\newpage

## Counts

In order to normalise the raw counts we will start by determining the relative library sizes, or size factors for each library. For example, if the counts of the expressed genes in one sample are, on average, twice as high as in another, the size factor for the first sample should be twice as large as the one for the other sample. These size factors can be obtained with the function `estimateSizeFactors`:

```{r counts, }
dds=estimateSizeFactors(ddsHTSeq)
sizeFactors(dds)

#Plot column sums according to size factor
plot(sizeFactors(dds), colSums(counts(ddsHTSeq)))
abline(lm(colSums(counts(dds)) ~ sizeFactors(dds) + 0))
text(sizeFactors(dds), colSums(counts(ddsHTSeq)), colnames(counts(dds)), pos=3)
dev.off()
```

### Raw vs Normalized Counts
Once we have this information, the normalised data is obtained by dividing each column of the count table by the corresponding size factor. We can perform this calculation by calling the function counts with a the `normalized` argument set as `TRUE`. Since we won't be normalizing this data, we'll set it as `FALSE`

#### Normalized

```{r counts_norm, }
library("vioplot")
dds.ncounts=counts(dds, normalized=TRUE)
head(dds.ncounts)
countData=data.frame(dds.ncounts)
vioplot(countData$Day_1,countData$Day_2, countData$Day_3, countData$Night_4, countData$Night_5, countData$Night_6,
        col="gold",
        names=colnames(countData))
title("Violin Plots for Normalized Counts")
summary(countData)
normalized.countData=countData
```

\newpage

#### Raw

```{r counts_no_norm, }
library("vioplot")
dds.ncounts=counts(dds, normalized=FALSE)
head(dds.ncounts)
countData=data.frame(dds.ncounts)
vioplot(countData$Day_1,countData$Day_2, countData$Day_3, countData$Night_4, countData$Night_5, countData$Night_6,
        col="gold",
        names=colnames(countData))
title("Violin Plots for Raw Counts")
summary(countData)
```

\newpage

## Rank Abundance
Plotting Rank Abundance for top `r topN` genes.

### Day Counts
```{r day_rank}
library("tidyr")
library("dplyr")
# Comment the following line out if normalized counts not required.
countData=normalized.countData
day.counts.ordered = countData %>% 
	select(starts_with("Day")) %>% 
	mutate(Name=rownames(.)) %>% 
  gather(ToD, Counts, -Name) %>%
  group_by(Name) %>%
  summarise(Totals=sum(Counts), Mean=mean(Counts), SE=sd(Counts)/sqrt(length(Counts))) %>%
	top_n(topN, Mean) %>% 
	mutate(Rank=min_rank(desc(Mean)))

ggplot(day.counts.ordered, aes(Rank,Mean)) +
  geom_point(
  	#aes(order=Rank)
  	) + 
	xlim(1,topN) +
	geom_errorbar(aes(ymax=Mean+SE, ymin=Mean-SE)) +
  scale_y_log10()+
  ylab("log10 Average Counts")+
  xlab("Top 500 Genes")+
	theme_minimal()
```

### Night Counts
```{r night_rank}
library("tidyr")
library("dplyr")
night.counts.ordered = countData %>% 
	select(starts_with("Night")) %>% 
	mutate(Name=rownames(.)) %>% 
  gather(ToD, Counts, -Name) %>%
  group_by(Name) %>%
  summarise(Totals=sum(Counts), Mean=mean(Counts), SE=sd(Counts)/sqrt(length(Counts))) %>%
	top_n(topN, Mean) %>% 
	mutate(Rank=min_rank(desc(Mean)))

ggplot(night.counts.ordered, aes(Rank,Mean)) +
  geom_point(
  #	aes(order=Rank)
  	) +
	geom_errorbar(aes(ymax=Mean+SE, ymin=Mean-SE)) +
  scale_y_log10() +
	xlim(1,topN)+
  ylab("log10 Average Counts")+
  xlab("Top 500 Genes")+
	theme_minimal()
```

### Day vs Night
```{r d_vs_n_rank}
day.counts.ordered$Sample="Day"
night.counts.ordered$Sample="Night"
#combined.counts=rbind(day.counts.ordered,night.counts.ordered)
combined.counts=bind_rows(list(day.counts.ordered,night.counts.ordered)) %>%
	select(-Rank) %>% 
  top_n(topN, Mean) %>% 
	mutate(Rank=min_rank(desc(Mean)))

ggplot(combined.counts, aes(Rank,Mean, color=Sample)) +
  geom_point(
#  	aes(order=Rank)
  	) + 
	geom_errorbar(aes(ymax=Mean+SE, ymin=Mean-SE)) +
	xlim(1,topN) +
  scale_y_log10() +
	scale_color_brewer(palette = "Dark2") +
  ylab("log10 Average Counts")+
  xlab("Combined Top Genes")+
	theme_minimal()
```

```{r annotate_counts, cache=TRUE}
annotation=read.table("data/all_combined.products", sep="\t", quote="")
colnames(annotation)=c("Name", "IMG_Product","IMG_Source")
annotation$Name=as.character(annotation$Name)

combined.union=as.data.frame(union(day.counts.ordered$Name,night.counts.ordered$Name))
colnames(combined.union)="Name"
combined.union$Name=as.character(combined.union$Name)

tmp.countData=countData
tmp.countData$Name=rownames(tmp.countData)

combined.union=left_join(combined.union, tmp.countData, by="Name")
combined.union=left_join(combined.union, day.counts.ordered, by="Name")
combined.union=left_join(combined.union, night.counts.ordered, by="Name")

colnames(combined.union)=c("Name","Day_1","Day_2","Day_3","Night_4","Night_5","Night_6",
                            "Day_Totals","Day_Mean","Day_SE","Day_Rank", "Del1",
                            "Night_Totals","Night_Mean","Night_SE", "Night_Rank", "Del2")
combined.union=combined.union %>% select(-c(Del1,Del2))
combined.anno=left_join(combined.union, annotation, by="Name")
```

```{r write_counts}
write.table(combined.anno, file="data/combined_raw_counts.tsv",sep="\t", quote=FALSE, row.names = FALSE, na = "0");
```

\newpage

## Differential Expression

Differential expression was calculater using the DESeq2 wrapper function over 4 processors.

```{r deseq_wrapper,  }
library("BiocParallel")
register(MulticoreParam(4))
dds <- DESeq(dds, parallel=T)
```

### Removing Batch Effects

Using package `sva`. Here is how the package has been described:

> The sva package contains functions for removing batch effects and other unwanted variation in high-throughput experiment. Specifically, the sva package contains functions for the identifying and building surrogate variables for high-dimensional data sets. Surrogate variables are covariates constructed directly from high-dimensional data (like gene expression/RNA sequencing/methylation/brain imaging data) that can be used in subsequent analyses to adjust for unknown, unmodeled, or latent sources of noise. The sva package can be used to remove artifacts in three ways: (1) identifying and estimating surrogate variables for unknown sources of variation in high-throughput experiments (Leek and Storey 2007 PLoS Genetics,2008 PNAS), (2) directly removing known batch effects using ComBat (Johnson et al. 2007 Biostatistics) and (3) removing batch effects with known control probes (Leek 2014 biorXiv). Removing batch effects and using surrogate variables in differential expression analysis have been shown to reduce dependence, stabilize error rate estimates, and improve reproducibility, see (Leek and Storey 2007 PLoS Genetics, 2008 PNAS or Leek et al. 2011 Nat. Reviews Genetics).


```{r}
dat <- counts(dds, normalized=TRUE)
idx <- rowMeans(dat) > 1
dat <- dat[idx,]
mod <- model.matrix(~ condition, colData(dds))
mod0 <- model.matrix(~ 1, colData(dds))
library(sva)
svseq <- svaseq(dat, mod, mod0, n.sv=2)
ddsva=dds
ddsva$SV1=svseq$sv[, 1]
ddsva$SV2=svseq$sv[, 2]
design(ddsva)= ~SV1 + SV2 + condition
ddsva <- DESeq(ddsva)
```

## Results before removing batch effects

As `res` is a DataFrame object, it carries metadata with information on the meaning of the columns:

```{r results,  }
res <- results(dds, parallel=T)
mcols(res, use.names=TRUE)
summary(res)
```

## Results after removing batch effects
```{r results_wo_BE,  }
resva <- results(ddsva, parallel=T)
mcols(resva, use.names=TRUE)
summary(resva)
res=resva
dds=ddsva
```


### Multiple testing

Novices in high-throughput biology often assume that thresholding these p values at a low value, say 0.05, as is often done in other settings, would be appropriate – but it is not. We briefly explain why:
There are `r sum(res$pvalue < 0.05, na.rm=TRUE)` genes with a p value below 0.05 among the `r sum(!is.na(res$pvalue))` genes, for which the test succeeded in reporting a p value.

Now, assume for a moment that the null hypothesis is true for all genes, i.e., no gene is affected by the treatment with dexamethasone. Then, by the definition of p value, we expect up to `5%` of the genes to have a p value below 0.05. This amounts to `r floor(sum(!is.na(res$pvalue)) * 0.05)` genes. If we just considered the list of genes with a p value below 0.05 as differentially expressed, this list should therefore be expected to contain up to `r sum(!is.na(res$pvalue)) * 0.05`/`r sum(res$pvalue < 0.05, na.rm=TRUE)`=`r (sum(!is.na(res$pvalue)) * 0.05)/sum(res$pvalue < 0.05, na.rm=TRUE)*100`% false positives.

DESeq2 uses the Benjamini-Hochberg (BH) adjustment as described in the base R p.adjust function; in brief, this method calculates for each gene an adjusted p value which answers the following question: if one called significant all genes with a p value less than or equal to this gene’s p value threshold, what would be the fraction of false positives (the false discovery rate, FDR) among them (in the sense of the calculation outlined above)? These values, called the BH-adjusted p values, are given in the column padj of the res object.
Hence, if we consider a fraction of 10% false positives acceptable, we can consider all genes with an adjusted p value below 10% = 0.1 as significant. How many such genes are there?

```{r}
sigGenes=sum(res$padj < 0.1, na.rm=TRUE)
sigGenes
```

We subset the results table to these genes and then sort it by the log2 fold change estimate to get the significant genes with the strongest down-regulation.
```{r}
resSig <- subset(res, padj < 0.1)
head(resSig[ order( resSig$log2FoldChange ), ])
```

…and with the strongest upregulation. 
```{r}
# The order function gives the indices in increasing order, so a simple way to ask for decreasing order is to add a - sign. Alternatively, you can use the argument decreasing=TRUE.
head(resSig[ order( -resSig$log2FoldChange ), ])
```

\newpage
<!--
### Cook's Distance per gene

```{r cooks, }
W <- res$stat
maxCooks <- apply(assays(dds)[["cooks"]],1,max)
idx <- !is.na(W)
plot(rank(W[idx]), maxCooks[idx], xlab="rank of Wald statistic",
ylab="maximum Cook's distance per gene",
ylim=c(0,5), cex=.8, col=rgb(0,0,0))
m <- ncol(dds)
p <- 2
abline(h=qf(.99, p, m - p))
```
-->
\newpage

## Diagnostic Plots

### Plot Counts

A quick way to visualize the counts for a particular gene is to use the plotCounts function, which takes as arguments the DESeqDataSet, a gene name, and the group over which to plot the counts.

```{r plotCounts, }
topGene <- rownames(res)[which.min(res$padj)]
data <- plotCounts(dds, gene=topGene, intgroup="condition", returnData=TRUE)
ggplot(data, aes(x=condition, y=count, fill=condition)) +
  geom_dotplot(binaxis="y", stackdir="center")
```

\newpage

### MA-Plots

An "MA-plot" provides a useful overview for an experiment with a two-group comparison. The log2 fold change for a particular comparison is plotted on the y-axis and the average of the counts normalized by size factor is shown on the x-axis ("M" for minus, because a log ratio is equal to log minus log, and "A" for average).

```{r plotMA, }
plotMA(res, ylim=c(-5,5))
with(res[topGene, ], {
  points(baseMean, log2FoldChange, col="dodgerblue", cex=2, lwd=2)
  text(baseMean, log2FoldChange, topGene, pos=4, col="dodgerblue")
})
```

Each gene is represented with a dot. Genes with an adjusted p-value below a threshold (here 0.1, the default) are shown in red. The DESeq2 package incorporates a prior on log2 fold changes, resulting in moderated log2 fold changes from genes with low counts and highly variable counts, as can be seen by the narrowing of spread of points on the left side of the plot. This plot demonstrates that only genes with a large average normalized count contain sufficient information to yield a significant call.

\newpage

### Dispersion Estimataion

Whether a gene is called significant depends not only on its LFC but also on its within-group variability, which DESeq2 quantifies as the dispersion. For strongly expressed genes, the dispersion can be understood as a squared coefficient of variation: a dispersion value of 0.01 means that the gene's expression tends to differ by typically sqrt(0.01)=10% between samples of the same treatment group. For weak genes, the Poisson noise is an additional source of noise.

The function plotDispEsts visualizes DESeq2's dispersion estimates:

```{r disp_est, }
dds=estimateDispersions(dds)
plotDispEsts(dds)
```

The black points are the dispersion estimates for each gene as obtained by considering the information from each gene separately. Unless one has many samples, these values fluctuate strongly around their true values. Therefore, we fit the red trend line, which shows the dispersions' dependence on the mean, and then shrink each gene's estimate towards the red line to obtain the final estimates (blue points) that are then used in the hypothesis test. The blue circles above the main "cloud" of points are genes which have high gene-wise dispersion estimates which are labelled as dispersion outliers. These estimates are therefore not shrunk toward the fitted trend line.

\newpage

### P-Value Histogram

Another useful diagnostic plot is the histogram of the p values.

```{r pval_hist, }
hist(res$pvalue[res$baseMean > 1], breaks=20, col="grey50", border="white")
```

\newpage

## Gene clustering

In the sample distance heatmap made previously, the dendrogram at the side shows us a hierarchical clustering of the samples. Such a clustering can also be performed for the genes. Since the clustering is only relevant for genes that actually carry signal, one usually carries it out only for a subset of most highly variable genes. Here, for demonstration, let us select the 20 genes with the highest variance across samples. We will work with the rlog transformed counts:

```{r geneFilter, }
library("genefilter")
topVarGenes <- head(order(-rowVars(assay(rld))),20)
```

The heatmap becomes more interesting if we do not look at absolute expression strength but rather at the amount by which each gene deviates in a specific sample from the gene's average across all samples. Hence, we center each genes' values across samples, and plot a heatmap. We provide the column side colors to help identify the treated samples (in blue) from the untreated samples (in grey).

```{r clust_heat, }
colors <- colorRampPalette( rev(brewer.pal(9, "PuOr")) )(255)
sidecols <- c("grey","dodgerblue")[ rld$condition ]
mat <- assay(rld)[ topVarGenes, ]
mat <- mat - rowMeans(mat)

mat <- assay(rld)[ topVarGenes, ]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(rld))
pheatmap(mat, annotation_col=df)
```


We can now see blocks of genes which covary across patients. Note that a set of genes at the top of the heatmap are separating the N061011 cell line from the others. At the bottom of the heatmap, we see a set of genes for which the treated samples have higher gene expression.

\newpage

## Significant Genes

Total Significant genes: `r sigGenes`

```{r write, }
resOrdered <- res[order(res$padj),]
resOrdered$Name=rownames(resOrdered)

library(dplyr)
resOrdered.Sig=data.frame(resOrdered[1:sigGenes ,])
resOrdered.Sig=resOrdered.Sig[c(7,1:6)]
resOrdered.Sig=left_join(resOrdered.Sig, tmp.countData, by="Name")
resOrdered.Sig=left_join(resOrdered.Sig, day.counts.ordered, by="Name") %>% select(-c(Rank,Sample))
resOrdered.Sig=left_join(resOrdered.Sig, night.counts.ordered, by="Name") %>% select(-c(Rank,Sample))
resOrdered.Sig=left_join(resOrdered.Sig, annotation, by="Name")
colnames(resOrdered.Sig)[14:19]=c("Day_Total", "Day_Mean", "Day_SE", "Night_Total", "Night_Mean", "Night_SE")

write.table(as.data.frame(resOrdered.Sig), sep="\t",file="data/results.tsv", quote = FALSE, row.names =FALSE)

library(knitr)
resOrdered.Sig %>% 
	select(c(1,3,7,20,21)) %>% 
	kable(.)
```

\newpage
### Plot
```{r}
ranked.sig.data=resOrdered.Sig %>%
	arrange(padj,desc(abs(log2FoldChange))) %>% 
	mutate(Rank=row_number(padj))

ranked.sig.data %>% 
	ggplot(aes(Rank,log2FoldChange)) +
	xlab("Rank by adjusted p-values") +
	ylab("Log2 Fold Change") +
	geom_point() +
	geom_text(aes(label=IMG_Source), vjust=-4)+
	geom_errorbar(aes(ymax=log2FoldChange+lfcSE, ymin=log2FoldChange-lfcSE)) +
	geom_hline(aes(yintercept=0), linetype=2,color="red") +
	theme_minimal()
```

\newpage

## Independent Filtering

The MA plot highlights an important property of RNA-Seq data. For weakly expressed genes, we have no chance of seeing differential expression, because the low read counts suffer from so high Poisson noise that any biological effect is drowned in the uncertainties from the read counting. We can also show this by examining the ratio of small p values (say, less than, 0.01) for genes binned by mean normalized count:

```{r iFilter}
# create bins using the quantile function
qs <- c(0, quantile(res$baseMean[res$baseMean > 0], 0:7/7))
# cut the genes into the bins
bins <- cut(res$baseMean, qs)
# rename the levels of the bins using the middle point
levels(bins) <- paste0("~",round(.5*qs[-1] + .5*qs[-length(qs)]))
# calculate the ratio of $p$ values less than .01 for each bin
ratios <- tapply(res$pvalue, bins, function(p) mean(p < .01, na.rm=TRUE))
# plot these ratios
barplot(ratios, xlab="mean normalized count", ylab="ratio of small p values")
```

At first sight, there may seem to be little benefit in filtering out these genes. After all, the test found them to be non-significant anyway. However, these genes have an influence on the multiple testing adjustment, whose performance improves if such genes are removed. By removing the weakly-expressed genes from the input to the FDR procedure, we can find more genes to be significant among those which we keep, and so improved the power of our test. This approach is known as independent filtering.

\newpage

The DESeq2 software automatically performs independent filtering which maximizes the number of genes which will have adjusted p value less than a critical value (by default, alpha is set to 0.1). This automatic independent filtering is performed by, and can be controlled by, the results function. We can observe how the number of rejections changes for various cutoffs based on mean normalized count. The following optimal threshold and table of possible values is stored as an attribute of the results object.

```{r autoIFilter}
attr(res,"filterThreshold")

plot(attr(res,"filterNumRej"),type="b",
     xlab="quantiles of 'baseMean'",
     ylab="number of rejections")
```

The term independent highlights an important caveat. Such filtering is permissible only if the filter criterion is independent of the actual test statistic. Otherwise, the filtering would invalidate the test and consequently the assumptions of the BH procedure. This is why we filtered on the average over all samples: this filter is blind to the assignment of samples to the treatment and control group and hence independent. The independent filtering software used inside DESeq2 comes from the genefilter package, which contains a reference to a paper describing the statistical foundation for independent filtering.

\newpage

## Session Info

```{r session_info}
sessionInfo()
```
